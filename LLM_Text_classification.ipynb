{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drewhalfmann/drewhalfmann/blob/main/LLM_Text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHflbfyi7TdD"
      },
      "source": [
        "## Getting info about a csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQHC3kCo29nh"
      },
      "outputs": [],
      "source": [
        "## Getting info about a csv file\n",
        "\n",
        "# Read csv into dataframe\n",
        "filepath = '/content/drive/MyDrive/Presidents_Paper/CSV_files/Paragraphs.csv'\n",
        "df_results = pd.read_csv(filepath)\n",
        "\n",
        "#Display number of rows\n",
        "print(f\"Number of rows (excluding header): {len(df_results)}\")\n",
        "\n",
        "#Display column headings\n",
        "print(df_results.columns)\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "print(df_results.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize your pretrained model and tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('/content/drive/MyDrive/Presidents_Paper/Model_1800')\n",
        "model = RobertaForSequenceClassification.from_pretrained('/content/drive/MyDrive/Presidents_Paper/Model_1800')"
      ],
      "metadata": {
        "id": "OtO-OUqbMinB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hjx7WR7X3gh"
      },
      "source": [
        "# Health-Focus Analysis:\n",
        "\n",
        "\n",
        "1) Uses Roberta to give health confidence scores to\n",
        "speech titles and paragraphs\n",
        "2) Combines short paragraphs with nearby ones, in part based on similar confidence scores\n",
        "3) Determines health focus of each speech in a variety of ways, the best of which appears to be \"health similarity\"--calculates degree to which all paragraphs in a given speech are similar to paragraphs with high confidence scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjJJkPcA7YSq"
      },
      "source": [
        "## Part 1: Split Speech Text into Paragraphs and Create a New Dataset--Done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIm0sJUW3kuZ"
      },
      "outputs": [],
      "source": [
        "## Part 1: Split Speech Text into Paragraphs and Create a New Dataset\n",
        "\n",
        "# Import necessary libraries\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "import pandas as pd\n",
        "\n",
        "## Define paths\n",
        "dataset_path = '/content/drive/MyDrive/Presidents_Paper/Data_from_JupyterLab/All_noQ&A_250+NoSOTU.csv'\n",
        "output_path = '/content/drive/MyDrive/Presidents_Paper/CSV_files/All_noQ&A_250+NoSOTU_Paragraphs.csv'\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('csv', data_files=dataset_path)['train']\n",
        "\n",
        "# Ensure Speech_ID exists or create it\n",
        "if 'Speech_ID' not in dataset.column_names:\n",
        "    dataset = dataset.add_column('Speech_ID', range(1, len(dataset) + 1))\n",
        "\n",
        "# Function to split speeches into paragraphs and assign IDs\n",
        "def split_into_paragraphs(example, idx_start=1):\n",
        "    paragraphs = example['Speech'].split('\\n\\n')  # Adjust based on actual paragraph delimiters\n",
        "    return [{\n",
        "        'Speech_ID': example['Speech_ID'],\n",
        "        'Paragraph_ID': f\"{example['Speech_ID']}-{idx}\",\n",
        "        'Speaker': example['Speaker'],\n",
        "        'Date': example['Date'],\n",
        "        'Title': example['Title'],\n",
        "        'Paragraph_Text': paragraph.strip()\n",
        "    } for idx, paragraph in enumerate(paragraphs, start=idx_start)]\n",
        "\n",
        "# Apply the function and flatten the output\n",
        "paragraphs = [split_into_paragraphs(speech) for speech in dataset]\n",
        "flattened_paragraphs = [paragraph for speech_paragraphs in paragraphs for paragraph in speech_paragraphs]\n",
        "\n",
        "# Convert to pandas DataFrame, then to Dataset\n",
        "df = pd.DataFrame(flattened_paragraphs)\n",
        "final_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Export to CSV\n",
        "final_dataset.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Dataset with Speech_ID and Paragraph_ID created and saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGVPn9hA7ZPD"
      },
      "source": [
        "## Part 2: Split Paragraph dataset into six pieces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mbt6AfY1XLt",
        "outputId": "16a4bdb3-e0af-4fdf-e81a-885dbf1440ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input dataset contains 618093 rows.\n",
            "Part 1 saved to /content/drive/MyDrive/Presidents_Paper/CSV_files/All_noQ&A_250+NoSOTU_Paragraphs_part_1.csv contains 103015 rows.\n",
            "Part 2 saved to /content/drive/MyDrive/Presidents_Paper/CSV_files/All_noQ&A_250+NoSOTU_Paragraphs_part_2.csv contains 103015 rows.\n",
            "Part 3 saved to /content/drive/MyDrive/Presidents_Paper/CSV_files/All_noQ&A_250+NoSOTU_Paragraphs_part_3.csv contains 103015 rows.\n",
            "Part 4 saved to /content/drive/MyDrive/Presidents_Paper/CSV_files/All_noQ&A_250+NoSOTU_Paragraphs_part_4.csv contains 103015 rows.\n",
            "Part 5 saved to /content/drive/MyDrive/Presidents_Paper/CSV_files/All_noQ&A_250+NoSOTU_Paragraphs_part_5.csv contains 103015 rows.\n",
            "Part 6 saved to /content/drive/MyDrive/Presidents_Paper/CSV_files/All_noQ&A_250+NoSOTU_Paragraphs_part_6.csv contains 103018 rows.\n"
          ]
        }
      ],
      "source": [
        "# Part 2: Split Paragraph dataset into six pieces-not done\n",
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "csv_path = '/content/drive/MyDrive/Presidents_Paper/CSV_files/All_noQ&A_250+NoSOTU_Paragraphs.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Print the row count of the input dataset\n",
        "print(f'Input dataset contains {len(df)} rows.')\n",
        "\n",
        "# Calculate the size of each part\n",
        "part_size = len(df) // 6\n",
        "\n",
        "# Split the DataFrame into six parts and save each to a new CSV\n",
        "for i in range(6):\n",
        "    start_index = i * part_size\n",
        "    # For the last part, include any remaining rows\n",
        "    end_index = (i + 1) * part_size if i < 5 else len(df)\n",
        "    df_part = df.iloc[start_index:end_index]\n",
        "\n",
        "    # Construct the output path for each part\n",
        "    output_path = f'/content/drive/MyDrive/Presidents_Paper/CSV_files/All_noQ&A_250+NoSOTU_Paragraphs_part_{i+1}.csv'\n",
        "\n",
        "    # Save each part to a new CSV\n",
        "    df_part.to_csv(output_path, index=False)\n",
        "\n",
        "    # Print the row count of each output dataset\n",
        "    print(f'Part {i+1} saved to {output_path} contains {len(df_part)} rows.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "bk9IPaosV1Kh",
        "outputId": "ae3898f3-2c0a-4e3a-9492-18a2a92f7814"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b6d3fa732514>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# classification with distilbert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# classification with distilbert\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Ensure CUDA is used if available, and create a device object\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def print_cuda_memory_usage():\n",
        "    print(f\"Current CUDA Memory Usage:\")\n",
        "    print(f\"Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
        "    print(f\"Cached: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
        "\n",
        "# Paths and candidate labels\n",
        "dataset_path = '/content/drive/MyDrive/Presidents_Paper/CSV_files/All_noQ&A_250+NoSOTU_Paragraphs_part_1.csv'\n",
        "output_path = '/content/drive/MyDrive/Presidents_Paper/CSV_files/All_noQ&A_250+NoSOTU_Classified_Paragraphs_and Titles_part_1.csv'\n",
        "\n",
        "candidate_labels = ['health', 'prevention and health', 'drug and alcohol abuse', 'environmental health', 'safety and health', 'mental health', 'disability and health', 'occupational health', 'sexual health']\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "dataset = load_dataset('csv', data_files=dataset_path)['train']\n",
        "\n",
        "# Initialize the classifier with DistilBERT model\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
        "classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer, device=0)  # Ensure the device is correctly set\n",
        "\n",
        "# The rest of your code remains largely unchanged\n",
        "\n",
        "# Function to perform batch classification\n",
        "def classify_batch(batch, classifier, candidate_labels):\n",
        "    titles = batch['Title']\n",
        "    paragraphs = batch['Paragraph_Text']\n",
        "    # Process titles and paragraphs in batch for efficiency\n",
        "    title_results = classifier(titles, candidate_labels=candidate_labels, truncation=True)\n",
        "    paragraph_results = classifier(paragraphs, candidate_labels=candidate_labels, truncation=True)\n",
        "    return title_results, paragraph_results\n",
        "\n",
        "# Batch processing\n",
        "batch_size = 1024 # Adjust based on your GPU's memory capacity\n",
        "num_batches = len(dataset) // batch_size + (0 if len(dataset) % batch_size == 0 else 1)\n",
        "\n",
        "# Prepare data for storing results\n",
        "results = []\n",
        "\n",
        "# Print the number of batches\n",
        "print(f\"Number of batches: {num_batches}\")\n",
        "\n",
        "# Step 3: Classify the dataset\n",
        "start_time = time.time()  # Start time for runtime estimation\n",
        "\n",
        "# Print CUDA memory usage\n",
        "print_cuda_memory_usage()\n",
        "\n",
        "\n",
        "# Wrap the range with tqdm for a progress meter\n",
        "for i in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
        "    # Print CUDA memory usage before processing the batch\n",
        "    print(f\"Before processing batch {i+1}:\")\n",
        "    print_cuda_memory_usage()\n",
        "\n",
        "    batch = dataset.select(range(i*batch_size, min((i+1)*batch_size, len(dataset))))\n",
        "    title_results, paragraph_results = classify_batch(batch, classifier, candidate_labels)\n",
        "\n",
        "    for j, (title_res, paragraph_res) in enumerate(zip(title_results, paragraph_results)):\n",
        "        # Extract scores for each label\n",
        "        title_scores = {f\"{label}_Title_cs\": score for label, score in zip(title_res['labels'], title_res['scores'])}\n",
        "        paragraph_scores = {f\"{label}_Paragraph_cs\": score for label, score in zip(paragraph_res['labels'], paragraph_res['scores'])}\n",
        "\n",
        "        # Combine with original data\n",
        "        result_row = {**batch[j], **title_scores, **paragraph_scores}\n",
        "        results.append(result_row)\n",
        "\n",
        "for i in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
        "    # Print CUDA memory usage before processing the batch\n",
        "    print(f\"Before processing batch {i+1}:\")\n",
        "    print_cuda_memory_usage()\n",
        "\n",
        "    batch = dataset.select(range(i*batch_size, min((i+1)*batch_size, len(dataset))))\n",
        "    title_results, paragraph_results = classify_batch(batch, classifier, candidate_labels)\n",
        "\n",
        "end_time = time.time()  # End time for runtime estimation\n",
        "total_time = end_time - start_time  # Total runtime\n",
        "print(f\"Total processing time: {total_time:.2f} seconds\")\n",
        "print_cuda_memory_usage()\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "print(f\"Number of rows (excluding header): {len(df_results)}\")\n",
        "\n",
        "# Display the columns\n",
        "print(df_results.columns)\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "print(df_results.head())\n",
        "\n",
        "# Step 6: Export to CSV\n",
        "df_results.to_csv(output_path, index=False)\n",
        "print(f\"Classification completed. Results saved to {output_path}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, DistilBertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"AyoubChLin/distilbert_ag_cnn\")\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"AyoubChLin/distilbert_ag_cnn\")\n",
        "\n",
        "# Replace \"input_text\" with the actual news article\n",
        "input_text = \"The stock market had a strong performance today, driven by strong earnings reports from technology companies.\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "# Make a prediction\n",
        "outputs = model(**inputs)\n",
        "predicted_label = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "# Print the predicted label\n",
        "print(predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELh-13C1BNZi",
        "outputId": "46d37ce0-9372-4790-ffcd-e65edd8e43f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PghJDvrWlVGI"
      },
      "source": [
        "## Part 3: Uses Roberta to give health confidence scores to speech titles and paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "q9ZBMwYpr6eo",
        "outputId": "50caa29c-b9cf-4f04-a6f5-bef242da89d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of batches: 101\n",
            "Current CUDA Memory Usage:\n",
            "Allocated: 13.25 GB\n",
            "Cached: 13.30 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing batches:   0%|          | 0/101 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before processing batch 1:\n",
            "Current CUDA Memory Usage:\n",
            "Allocated: 13.25 GB\n",
            "Cached: 13.30 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing batches:   1%|          | 1/101 [05:46<9:37:15, 346.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before processing batch 2:\n",
            "Current CUDA Memory Usage:\n",
            "Allocated: 13.25 GB\n",
            "Cached: 13.33 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing batches:   2%|▏         | 2/101 [11:34<9:33:30, 347.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before processing batch 3:\n",
            "Current CUDA Memory Usage:\n",
            "Allocated: 13.25 GB\n",
            "Cached: 13.33 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing batches:   3%|▎         | 3/101 [17:24<9:29:01, 348.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before processing batch 4:\n",
            "Current CUDA Memory Usage:\n",
            "Allocated: 13.25 GB\n",
            "Cached: 13.33 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing batches:   4%|▍         | 4/101 [23:10<9:22:12, 347.76s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before processing batch 5:\n",
            "Current CUDA Memory Usage:\n",
            "Allocated: 13.25 GB\n",
            "Cached: 13.35 GB\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing batches:   4%|▍         | 4/101 [24:07<9:44:59, 361.85s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-54ca7c42b13e>\u001b[0m in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mtitle_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraph_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtitle_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraph_res\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraph_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-54ca7c42b13e>\u001b[0m in \u001b[0;36mclassify_batch\u001b[0;34m(batch, classifier, candidate_labels)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mparagraphs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Paragraph_Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Process titles and paragraphs in batch for efficiency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mtitle_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcandidate_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mparagraph_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcandidate_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtitle_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraph_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/zero_shot_classification.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unable to understand extra arguments {args}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"This example is {}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1119\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m                 )\n\u001b[0;32m-> 1121\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/zero_shot_classification.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"use_cache\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"use_cache\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         model_outputs = {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1199\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;31m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0;31m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_extended_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;31m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mget_extended_attention_mask\u001b[0;34m(self, attention_mask, input_shape, device, dtype)\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;31m# effectively the same as removing these entirely.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0mextended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# fp16 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m         \u001b[0mextended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "## Part 3. Classify Title and Paragraph (Repeat 6 times)\n",
        "\n",
        "# Step 1: Install and import necessary libraries\n",
        "# !pip install transformers datasets torch tqdm\n",
        "\n",
        "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm  # Import tqdm for progress metering\n",
        "import time  # Import time to measure runtime\n",
        "\n",
        "# Ensure CUDA is used if available, and create a device object\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def print_cuda_memory_usage():\n",
        "    print(f\"Current CUDA Memory Usage:\")\n",
        "    print(f\"Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")  # Note: removed device argument for simplicity\n",
        "    print(f\"Cached: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
        "\n",
        "# Paths and candidate labels\n",
        "dataset_path = '/content/drive/MyDrive/Presidents_Paper/CSV_files/All_noQ&A_250+NoSOTU_Paragraphs_part_1.csv'\n",
        "output_path = '/content/drive/MyDrive/Presidents_Paper/CSV_files/All_noQ&A_250+NoSOTU_Classified_Paragraphs_and Titles_part_1.csv'\n",
        "\n",
        "\n",
        "candidate_labels = ['health', 'prevention and health', 'drug and alcohol abuse', 'environmental health', 'safety and health', 'mental health', 'disability and health', 'occupational health', 'sexual health']\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "dataset = load_dataset('csv', data_files=dataset_path)['train']\n",
        "\n",
        "# Initialize the classifier with the roberta-large-mnli model\n",
        "model_name = \"roberta-large-mnli\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
        "classifier = pipeline(\"zero-shot-classification\", model=model, tokenizer=tokenizer, device=device)\n",
        "\n",
        "# Function to perform batch classification\n",
        "def classify_batch(batch, classifier, candidate_labels):\n",
        "    titles = batch['Title']\n",
        "    paragraphs = batch['Paragraph_Text']\n",
        "    # Process titles and paragraphs in batch for efficiency\n",
        "    title_results = classifier(titles, candidate_labels=candidate_labels, truncation=True)\n",
        "    paragraph_results = classifier(paragraphs, candidate_labels=candidate_labels, truncation=True)\n",
        "    return title_results, paragraph_results\n",
        "\n",
        "# Batch processing\n",
        "batch_size = 1024 # Adjust based on your GPU's memory capacity\n",
        "num_batches = len(dataset) // batch_size + (0 if len(dataset) % batch_size == 0 else 1)\n",
        "\n",
        "# Prepare data for storing results\n",
        "results = []\n",
        "\n",
        "# Print the number of batches\n",
        "print(f\"Number of batches: {num_batches}\")\n",
        "\n",
        "# Step 3: Classify the dataset\n",
        "start_time = time.time()  # Start time for runtime estimation\n",
        "\n",
        "# Print CUDA memory usage\n",
        "print_cuda_memory_usage()\n",
        "\n",
        "\n",
        "# Wrap the range with tqdm for a progress meter\n",
        "for i in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
        "    # Print CUDA memory usage before processing the batch\n",
        "    print(f\"Before processing batch {i+1}:\")\n",
        "    print_cuda_memory_usage()\n",
        "\n",
        "    batch = dataset.select(range(i*batch_size, min((i+1)*batch_size, len(dataset))))\n",
        "    title_results, paragraph_results = classify_batch(batch, classifier, candidate_labels)\n",
        "\n",
        "    for j, (title_res, paragraph_res) in enumerate(zip(title_results, paragraph_results)):\n",
        "        # Extract scores for each label\n",
        "        title_scores = {f\"{label}_Title_cs\": score for label, score in zip(title_res['labels'], title_res['scores'])}\n",
        "        paragraph_scores = {f\"{label}_Paragraph_cs\": score for label, score in zip(paragraph_res['labels'], paragraph_res['scores'])}\n",
        "\n",
        "        # Combine with original data\n",
        "        result_row = {**batch[j], **title_scores, **paragraph_scores}\n",
        "        results.append(result_row)\n",
        "\n",
        "for i in tqdm(range(num_batches), desc=\"Processing batches\"):\n",
        "    # Print CUDA memory usage before processing the batch\n",
        "    print(f\"Before processing batch {i+1}:\")\n",
        "    print_cuda_memory_usage()\n",
        "\n",
        "    batch = dataset.select(range(i*batch_size, min((i+1)*batch_size, len(dataset))))\n",
        "    title_results, paragraph_results = classify_batch(batch, classifier, candidate_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "end_time = time.time()  # End time for runtime estimation\n",
        "total_time = end_time - start_time  # Total runtime\n",
        "print(f\"Total processing time: {total_time:.2f} seconds\")\n",
        "print_cuda_memory_usage()\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "print(f\"Number of rows (excluding header): {len(df_results)}\")\n",
        "\n",
        "# Display the columns\n",
        "print(df_results.columns)\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "print(df_results.head())\n",
        "\n",
        "# Step 6: Export to CSV\n",
        "df_results.to_csv(output_path, index=False)\n",
        "print(f\"Classification completed. Results saved to {output_path}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB2VHhzW7c6x"
      },
      "source": [
        "## Part 4: Aggregate six classified paragraph files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HPglHAWmXmp"
      },
      "outputs": [],
      "source": [
        "#Part 4: Aggregate six classified paragraph files\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Base path and file pattern\n",
        "base_path = '/content/drive/MyDrive/Presidents_Paper/CSV_files/'\n",
        "file_pattern = 'All_noQ&A_250+NoSOTU_Classified_Paragraphs_and Titles_part_{}.csv'\n",
        "\n",
        "# Initialize an empty list to hold DataFrames\n",
        "dfs = []\n",
        "\n",
        "# Loop through the six parts\n",
        "for i in range(1, 7):\n",
        "    file_path = base_path + file_pattern.format(i)\n",
        "    # Read the CSV file and append the DataFrame to the list\n",
        "    df = pd.read_csv(file_path)\n",
        "    dfs.append(df)\n",
        "    # Print the row count for the current part\n",
        "    print(f'Loaded {len(df)} rows from \"{file_path}\"')\n",
        "\n",
        "# Concatenate all DataFrames in the list\n",
        "df_concatenated = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Save the concatenated DataFrame to a new CSV file\n",
        "output_path = base_path + 'Aggregated_Classified_Paragraphs_and_Titles.csv'\n",
        "df_concatenated.to_csv(output_path, index=False)\n",
        "\n",
        "# Print the total row count of the aggregated DataFrame\n",
        "print(f'Aggregated dataset saved to \"{output_path}\" contains {len(df_concatenated)} rows.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkzbvqfRlaKH"
      },
      "source": [
        "## Part 5: Combines small paragraphs with nearby ones. The new larger paragraph takes the confidence score of its highest-scored member"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4X2ePiWSci2"
      },
      "outputs": [],
      "source": [
        "# Part 5: Combines small paragraphs with nearby ones. The new larger paragraph takes the confidence score of its highest-scored member\n",
        "import pandas as pd\n",
        "\n",
        "# Function to aggregate paragraphs\n",
        "def aggregate_paragraphs(classified_df, confidence_threshold, length_threshold):\n",
        "    rows_to_concat = []\n",
        "    current_paragraph_or_title = \"\"\n",
        "    current_confidence_scores = []\n",
        "    current_component_paragraphs = []\n",
        "    skip_until_index = -1\n",
        "\n",
        "    for i, row in classified_df.iterrows():\n",
        "        text, score, paragraph_type = row['Text'], row['Score'], row['Type']\n",
        "\n",
        "        if paragraph_type != 'Paragraph':\n",
        "            rows_to_concat.append(row)\n",
        "            continue\n",
        "\n",
        "        if i <= skip_until_index:\n",
        "            continue\n",
        "\n",
        "        if len(text.split()) < length_threshold and score >= confidence_threshold:\n",
        "            current_paragraph_or_title += (\" \" if current_paragraph_or_title else \"\") + text\n",
        "            current_confidence_scores.append(score)\n",
        "            current_component_paragraphs.append(text)\n",
        "\n",
        "            for j in range(i+1, min(i+3, len(classified_df))):\n",
        "                next_row = classified_df.iloc[j]\n",
        "                if next_row['Type'] != 'Paragraph':\n",
        "                    break\n",
        "                next_text, next_score = next_row['Text'], next_row['Score']\n",
        "                if len(next_text.split()) < length_threshold and next_score >= confidence_threshold:\n",
        "                    current_paragraph_or_title += \" \" + next_text\n",
        "                    current_confidence_scores.append(next_score)\n",
        "                    current_component_paragraphs.append(next_text)\n",
        "                    skip_until_index = j\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "        if current_paragraph_or_title:\n",
        "            aggregated_row = row.copy()\n",
        "            aggregated_row['Text'] = current_paragraph_or_title\n",
        "            aggregated_row['Score'] = max(current_confidence_scores)\n",
        "            aggregated_row['Aggregated'] = True\n",
        "            aggregated_row['Component_Paragraphs'] = ' || '.join(current_component_paragraphs)\n",
        "\n",
        "            rows_to_concat.append(aggregated_row)\n",
        "\n",
        "            current_paragraph_or_title = \"\"\n",
        "            current_confidence_scores = []\n",
        "            current_component_paragraphs = []\n",
        "        else:\n",
        "            rows_to_concat.append(row)\n",
        "\n",
        "    # Create the aggregated DataFrame using concat\n",
        "    aggregated_df = pd.concat(rows_to_concat, axis=1).T\n",
        "\n",
        "    # Rename and remove columns as per your request\n",
        "    aggregated_df = aggregated_df.drop(columns=['Original_Title'])\n",
        "    aggregated_df = aggregated_df.rename(columns={\n",
        "        'SpeechID': 'Speech_ID',\n",
        "        'Text': 'Paragraph_Or_Title',\n",
        "        'Score': 'Confidence_Score'\n",
        "    })\n",
        "\n",
        "    return aggregated_df\n",
        "\n",
        "# Load the classified text DataFrame\n",
        "classified_df = pd.read_csv('/content/drive/MyDrive/Presidents_Paper/CSV_files/Classified_Paragraphs_and_Titles.csv')\n",
        "\n",
        "# Set thresholds\n",
        "confidence_threshold = 0.5\n",
        "length_threshold = 90\n",
        "\n",
        "# Aggregate the paragraphs\n",
        "aggregated_df = aggregate_paragraphs(classified_df, confidence_threshold, length_threshold)\n",
        "\n",
        "# Save the data to a new CSV file\n",
        "aggregated_paragraphs_csv_path = '/content/drive/MyDrive/Presidents_Paper/CSV_files/Aggregated_Short_Paragraphs.csv'\n",
        "aggregated_df.to_csv(aggregated_paragraphs_csv_path, index=False)\n",
        "\n",
        "# Print confirmation\n",
        "print(\"Aggregated paragraphs saved to:\", aggregated_paragraphs_csv_path)\n",
        "\n",
        "# Print columns and rows of DataFrame\n",
        "first_rows = aggregated_df.head(2)\n",
        "columns = aggregated_df.columns\n",
        "print(columns)\n",
        "print(first_rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tt8qt28I7FIU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JtQWK8E2sG7"
      },
      "source": [
        "## Part 6: Health Focus Analysis--Calculates Hlth_Similarity (best one, Normalized_Hlth_Confidence, Proportion_Hlth_Graphs\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRq06BAJ6tMA"
      },
      "source": [
        " Hlth_Similarity:\n",
        "        This measure represents how similar each speech is to a standard health-focused embedding.\n",
        "        The standard health-focused embedding is created by averaging embeddings of paragraphs with a high confidence score (greater than 0.9) in health topics.\n",
        "        Cosine similarity is used to compare the embedding of each speech with this standard embedding.\n",
        "        A higher Hlth_Similarity score indicates that the speech's content is more closely aligned with typical health-related topics.\n",
        "\n",
        "    Normalized_Hlth_Confidence:\n",
        "        This is a normalized measure of the emphasis on health topics within each speech.\n",
        "        It's calculated as the weighted sum of confidence scores for health-related paragraphs in a speech, normalized by the total sum of confidence scores across all paragraphs in that speech.\n",
        "        The weighting is done by squaring the confidence score for each paragraph, emphasizing paragraphs with higher confidence.\n",
        "        This measure accounts for both the quantity and the confidence of health-related content, providing a more nuanced view than simply counting paragraphs.\n",
        "\n",
        "    Proportion_Hlth_Graphs:\n",
        "        This measure calculates the proportion of paragraphs in each speech that are health-related.\n",
        "        It's determined by dividing the number of paragraphs with a confidence score above a set threshold by the total number of paragraphs in the speech.\n",
        "    \n",
        "\n",
        "    Unnormalized_Hlth_Confidence:\n",
        "        This is an unnormalized index representing the total confidence scores of health-related paragraphs in each speech.\n",
        "        It sums the confidence scores of paragraphs in a speech where the score exceeds the confidence threshold, without any normalization.\n",
        "        This measure indicates the overall presence of health-related content in a speech but doesn't account for the total amount of content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "j1SsrO40SnXp",
        "outputId": "11b83459-db7c-4d12-935b-66a913131b67"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sentence_transformers'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-73cb5483dca5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Health Focus Analysis--Calculates 'Hlth_Similarity','Normalized_Hlth_Confidence','Proportion_Hlth_Graphs'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Health Focus Analysis--Calculates 'Hlth_Similarity','Normalized_Hlth_Confidence','Proportion_Hlth_Graphs'\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.spatial.distance import cosine\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset containing paragraphs and their confidence scores\n",
        "file_path = '/content/drive/MyDrive/Presidents_Paper/CSV_files/Aggregated_Short_Paragraphs.csv'\n",
        "df_paragraphs = pd.read_csv(file_path)\n",
        "\n",
        "# Set the confidence threshold\n",
        "confidence_threshold = 0.5\n",
        "\n",
        "# Filter paragraphs with a Confidence_Score > 0.9 for health embedding calculation\n",
        "health_paragraphs = df_paragraphs[df_paragraphs['Confidence_Score'] > 0.9]['Paragraph_Or_Title']\n",
        "\n",
        "# Initialize the pre-trained model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for the filtered health paragraphs\n",
        "health_embeddings = [model.encode(paragraph) for paragraph in health_paragraphs]\n",
        "standard_health_embedding = np.mean(health_embeddings, axis=0)\n",
        "\n",
        "# Function to calculate cosine similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return 1 - cosine(vec1, vec2)\n",
        "\n",
        "# Generate embeddings for each speech\n",
        "speech_embeddings = {}\n",
        "for speech_id, group in df_paragraphs.groupby('Speech_ID'):\n",
        "    concatenated_text = \" \".join(group['Paragraph_Or_Title'])\n",
        "    speech_embeddings[speech_id] = model.encode(concatenated_text)\n",
        "\n",
        "# Calculate similarity of each speech to the standard health embedding\n",
        "for speech_id in df_paragraphs['Speech_ID'].unique():\n",
        "    df_paragraphs.loc[df_paragraphs['Speech_ID'] == speech_id, 'Hlth_Similarity'] = cosine_similarity(speech_embeddings[speech_id], standard_health_embedding)\n",
        "\n",
        "# Filter paragraphs based on the confidence threshold for index calculations\n",
        "significant_health_paragraphs = df_paragraphs[df_paragraphs['Confidence_Score'] >= confidence_threshold]\n",
        "\n",
        "# Calculate the proportion of health-related paragraphs\n",
        "proportion_health_paragraphs = significant_health_paragraphs.groupby('Speech_ID').size() / df_paragraphs.groupby('Speech_ID').size()\n",
        "proportion_health_paragraphs = proportion_health_paragraphs.reset_index(name='Proportion_Hlth_Graphs')\n",
        "\n",
        "# Calculate the unnormalized health emphasis index\n",
        "unnormalized_health_index = significant_health_paragraphs.groupby('Speech_ID')['Confidence_Score'].sum().reset_index(name='Unnormalized_Hlth_Confidence')\n",
        "\n",
        "# Calculate the weighted sum of confidence scores for each paragraph\n",
        "df_paragraphs['Weighted_Confidence'] = df_paragraphs['Confidence_Score'] * df_paragraphs['Confidence_Score']\n",
        "\n",
        "# Calculate the total sum of confidence scores for normalization\n",
        "total_confidence = df_paragraphs.groupby('Speech_ID')['Confidence_Score'].sum()\n",
        "\n",
        "# Calculate the normalized health emphasis index\n",
        "normalized_health_index = df_paragraphs.groupby('Speech_ID').apply(\n",
        "    lambda x: x['Weighted_Confidence'].sum() / total_confidence[x.name]\n",
        ").reset_index(name='Normalized_Hlth_Confidence')\n",
        "\n",
        "# Combine the calculated indices with the df_paragraphs at the speech level\n",
        "speech_level_df = df_paragraphs.drop_duplicates(subset='Speech_ID').copy()\n",
        "speech_level_df = speech_level_df.merge(proportion_health_paragraphs, on='Speech_ID', how='left')\n",
        "speech_level_df = speech_level_df.merge(unnormalized_health_index, on='Speech_ID', how='left')\n",
        "speech_level_df = speech_level_df.merge(normalized_health_index, on='Speech_ID', how='left')\n",
        "\n",
        "# Fill NaN values with 0 for speeches with no significant health content\n",
        "speech_level_df['Proportion_Hlth_Graphs'].fillna(0, inplace=True)\n",
        "speech_level_df['Unnormalized_Hlth_Confidence'].fillna(0, inplace=True)\n",
        "speech_level_df['Normalized_Hlth_Confidence'].fillna(0, inplace=True)\n",
        "\n",
        "# Select only the relevant columns for the final CSV\n",
        "final_columns = ['Speech_ID', 'Speaker', 'Date', 'Title', 'Speech', 'Label', 'Hlth_Similarity','Normalized_Hlth_Confidence',\n",
        "                 'Proportion_Hlth_Graphs', 'Unnormalized_Hlth_Confidence'\n",
        "                ]\n",
        "final_df = speech_level_df[final_columns]\n",
        "final_df = final_df.round(2)\n",
        "\n",
        "# Save the final analysis results to a CSV file\n",
        "final_analysis_csv_path = '/content/drive/MyDrive/Presidents_Paper/CSV_files/Health_Focus_Combined.csv'\n",
        "final_df.to_csv(final_analysis_csv_path, index=False)\n",
        "\n",
        "print(\"Results saved to CSV.\")\n",
        "columns = final_df.columns\n",
        "print(columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWPtkKbATSOp"
      },
      "source": [
        "# Program to Evaluate Fine-tuning Data\n",
        "\"We conducted a data quality analysis that included: (1) exploring data structure and characteristics, (2) identifying missing values, duplicates, and potential noise, (3) examining metadata like authorship, publication dates, and sources to understand dataset origins and biases, and (4) evaluating label quality by analyzing class distribution and identifying underrepresented labels.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiKyb6_JRNIk"
      },
      "outputs": [],
      "source": [
        "# Program to Evaluate Fine-tuning Data\n",
        "\n",
        "# Importing Necessary Libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "\n",
        "# Count rows in each part of dataset\n",
        "\n",
        "def calculate_subset_rows_with_labels(dataset_name):\n",
        "    \"\"\"\n",
        "    Calculate and print the number of rows in each subset (training, testing, and validation)\n",
        "    of a Hugging Face dataset, along with the total number of rows, and list the label names.\n",
        "\n",
        "    Args:\n",
        "    dataset_name (str): The name of the dataset on Hugging Face.\n",
        "\n",
        "    Returns:\n",
        "    int: Total number of rows across all subsets.\n",
        "    \"\"\"\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset(dataset_name)\n",
        "\n",
        "    # Initialize total count\n",
        "    total_rows = 0\n",
        "\n",
        "    # Check and print the number of rows in each subset if it exists, along with label names\n",
        "    for subset in ['train', 'validation', 'test']:\n",
        "        if subset in dataset:\n",
        "            subset_rows = len(dataset[subset])\n",
        "            total_rows += subset_rows\n",
        "            print(f\"\\nNumber of rows in '{subset}' subset: {subset_rows}\")\n",
        "\n",
        "            # Print label names if the subset has a 'label' feature\n",
        "            if 'label' in dataset[subset].features:\n",
        "                label_names = dataset[subset].features['label'].names\n",
        "                print(f\"Label names in '{subset}' subset: {label_names}\")\n",
        "\n",
        "    # Print the total number of rows across all subsets\n",
        "    print(f\"\\nTotal number of rows across all subsets: {total_rows}\")\n",
        "    return total_rows\n",
        "\n",
        "def comprehensive_dataset_analysis(dataset_name, subset='train'):\n",
        "    \"\"\"\n",
        "    Perform a comprehensive analysis of a text classification dataset, including EDA,\n",
        "    data quality assessment, metadata and annotations analysis, and label quality checks with label names.\n",
        "\n",
        "    Args:\n",
        "    dataset_name (str): The name of the dataset on Hugging Face.\n",
        "    subset (str): The subset of the dataset to analyze (e.g., 'train', 'test', 'validation').\n",
        "\n",
        "    Returns:\n",
        "    None: This function prints out the analysis and shows plots.\n",
        "    \"\"\"\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    df = pd.DataFrame(dataset[subset])\n",
        "\n",
        "    # Retrieve label names if available\n",
        "    label_names = dataset[subset].features['label'].names if 'label' in dataset[subset].features else None\n",
        "    if label_names:\n",
        "        print(f\"Label names in '{subset}' subset: {label_names}\")\n",
        "\n",
        "    # Basic Exploratory Data Analysis (EDA)\n",
        "    print(f\"\\nBasic Exploratory Data Analysis (EDA) for '{subset}' subset:\")\n",
        "    print(\"First 5 Rows:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nBasic Statistical Summary:\")\n",
        "    print(df.describe(include='all'))\n",
        "    print(\"\\nInformation about Data Types and Missing Values:\")\n",
        "    print(df.info())\n",
        "\n",
        "    # Data Quality Assessment\n",
        "    print(\"\\nData Quality Assessment:\")\n",
        "    missing_values = df.isnull().sum()\n",
        "    print(f\"Missing Values in Each Column:\\n{missing_values}\")\n",
        "    duplicates = df.duplicated().sum()\n",
        "    print(f\"Number of Duplicate Rows: {duplicates}\")\n",
        "    if 'text' in df.columns:\n",
        "        noise_pattern = r'[^a-zA-Z0-9\\s,.!?]'\n",
        "        noisy_data = df['text'].str.contains(noise_pattern).sum()\n",
        "        print(f\"Number of Rows with Potential Noise in Text: {noisy_data}\")\n",
        "\n",
        "    # Metadata and Annotations Analysis\n",
        "    print(\"\\nMetadata and Annotations Analysis:\")\n",
        "    metadata_columns = ['author', 'timestamp', 'source']  # Example metadata columns\n",
        "    for col in metadata_columns:\n",
        "        if col in df.columns:\n",
        "            print(f\"\\nAnalysis of '{col}' Column:\")\n",
        "            print(f\"Unique values in '{col}':\\n{df[col].unique()}\")\n",
        "            print(f\"Value counts for '{col}':\\n{df[col].value_counts()}\")\n",
        "\n",
        "    # Label Quality Checks\n",
        "    print(\"\\nLabel Quality Checks:\")\n",
        "    if 'label' in df.columns:\n",
        "        class_counts = df['label'].value_counts()\n",
        "        print(f\"Class Distribution:\\n{class_counts}\")\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        class_counts.plot(kind='bar')\n",
        "        plt.xlabel('Classes')\n",
        "        plt.ylabel('Number of Samples')\n",
        "        plt.title('Class Distribution in the Dataset')\n",
        "        plt.show()\n",
        "        rare_threshold = 0.01 * len(df)\n",
        "        rare_labels = class_counts[class_counts < rare_threshold]\n",
        "        if not rare_labels.empty:\n",
        "            print(f\"Rare Labels (less than 1% of the dataset):\\n{rare_labels}\")\n",
        "        else:\n",
        "            print(\"No rare labels found based on the 1% threshold.\")\n",
        "    else:\n",
        "        print(\"No 'label' column found in the dataset.\")\n",
        "\n",
        "# Example Usage\n",
        "dataset_name = 'AyoubChLin/CNN_News_Articles_2011-2022'  # Replace with the actual dataset name\n",
        "total_rows = calculate_subset_rows_with_labels(dataset_name)\n",
        "comprehensive_dataset_analysis(dataset_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcq_ug2BhHcA"
      },
      "source": [
        "# Create Random Sample of Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StzqaHS9hGEP",
        "outputId": "24879521-0393-41c7-820c-5c235bcfb2cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled data saved to: /content/drive/MyDrive/Presidents_Paper/Data_from_JupyterLab/All_noQ&A_5.csv\n"
          ]
        }
      ],
      "source": [
        "# # Create Random Sample of Data\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/drive/MyDrive/Presidents_Paper/Data_from_JupyterLab/All_noQ&A.csv')\n",
        "\n",
        "# Randomly select 1000 speeches\n",
        "sampled_df = df.sample(n=5, random_state=42)\n",
        "\n",
        "# Save the sampled data to a new CSV file\n",
        "output_path = '/content/drive/MyDrive/Presidents_Paper/Data_from_JupyterLab/All_noQ&A_5.csv'\n",
        "sampled_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Sampled data saved to: {output_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi4_jeuvi4n6"
      },
      "source": [
        "# Program to assess OCR noise in dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygrhk2yQJSQd"
      },
      "outputs": [],
      "source": [
        "# Program to assess OCR noise in dataset\n",
        "# Importing Necessary Libraries\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_text_noise(dataset_name, subset='train'):\n",
        "    \"\"\"\n",
        "    Analyze potential noise in the text data of a dataset.\n",
        "\n",
        "    Args:\n",
        "    dataset_name (str): The name of the dataset on Hugging Face.\n",
        "    subset (str): The subset of the dataset to analyze (e.g., 'train', 'test', 'validation').\n",
        "\n",
        "    Returns:\n",
        "    None: This function prints out the analysis.\n",
        "    \"\"\"\n",
        "    # Load the dataset\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    df = pd.DataFrame(dataset[subset])\n",
        "\n",
        "    if 'text' not in df.columns:\n",
        "        print(\"No 'text' column found in the dataset.\")\n",
        "        return\n",
        "\n",
        "    # Define a regex pattern for potential noise\n",
        "    noise_pattern = re.compile(r'[^a-zA-Z0-9\\s,.!?]')\n",
        "\n",
        "    # Function to find noise in a text\n",
        "    def find_noise(text):\n",
        "        if not isinstance(text, str):\n",
        "            return []  # Return an empty list if the text is not a string\n",
        "        return noise_pattern.findall(text)\n",
        "\n",
        "    # Apply the function to each row in the text column\n",
        "    noise_list = df['text'].apply(find_noise)\n",
        "\n",
        "    # Flatten the list and count occurrences of each noise character\n",
        "    noise_counter = Counter([item for sublist in noise_list for item in sublist])\n",
        "\n",
        "    # Display the most common noise characters\n",
        "    print(\"Most Common Noise Characters and their Counts:\")\n",
        "    for noise, count in noise_counter.most_common():\n",
        "        print(f\"{noise}: {count}\")\n",
        "\n",
        "# Example Usage\n",
        "dataset_name = 'AyoubChLin/CNN_News_Articles_2011-2022'  # Replace with the actual dataset name\n",
        "analyze_text_noise(dataset_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rEQIbJqk8A7"
      },
      "source": [
        "# Pip Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psVmFRplkW_O"
      },
      "outputs": [],
      "source": [
        "! pip install LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3bghY4XrgXm"
      },
      "outputs": [],
      "source": [
        "! pip install gputil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvJ2wbwwSBez"
      },
      "outputs": [],
      "source": [
        "!pip show LDA | grep Location\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40cj3dG4mviR"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade pip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QjwYrGxmvNu"
      },
      "outputs": [],
      "source": [
        "! pip install numpy scipy scikit-learn joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtzTr51qmdOJ"
      },
      "outputs": [],
      "source": [
        "! pip install guidedlda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q89V-784PECk"
      },
      "outputs": [],
      "source": [
        "! pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWFNe5on3jtt"
      },
      "outputs": [],
      "source": [
        "! pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i58z6nt-Nv6D"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCYaSvLsPsLD"
      },
      "outputs": [],
      "source": [
        "!watch -n 1 nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJEWXVbVV9ua"
      },
      "outputs": [],
      "source": [
        "print(pd.read_csv('/content/drive/MyDrive/Presidents_Paper/Data_from_JupyterLab/All_noQ&A.csv').columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1xX751tyyIG"
      },
      "outputs": [],
      "source": [
        "! pip install GPUtil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOh1iy8zjdjO"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwk2IP6H5D_I"
      },
      "source": [
        "# Scrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duJe2Ob2Zxaz"
      },
      "outputs": [],
      "source": [
        "## Old version Part 1: Uses Roberta to give health confidence scores to speech titles and paragraphs\n",
        "## Remember to switch to GPU, remember to ! pip install transformers, sentence transformers, and datasets.\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from datasets import Dataset\n",
        "from tqdm.auto import tqdm  # for progress meter\n",
        "\n",
        "# Initialize the Zero-Shot Classifier with GPU\n",
        "def initialize_classifier():\n",
        "    try:\n",
        "        classifier = pipeline(\"zero-shot-classification\", model=\"roberta-large-mnli\", device=0)\n",
        "        return classifier\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing the classifier: {e}\")\n",
        "        return None\n",
        "\n",
        "# Classify text with detailed health categorization and multiple candidate labels\n",
        "def classify_text(text, classifier, candidate_labels):\n",
        "    result = classifier(text, candidate_labels)\n",
        "    best_label = result['labels'][0]\n",
        "    best_score = result['scores'][0]\n",
        "    return best_label, best_score\n",
        "\n",
        "# Specifying nuanced candidate labels\n",
        "candidate_labels = ['health', 'prevention and health', 'drug and alcohol abuse', 'environmental health', 'safety and health', 'mental health', 'disability and health', 'occupational health', 'sexual health']\n",
        "\n",
        "# Function to classify paragraphs and titles in batches with progress update\n",
        "def classify_paragraphs_and_titles(batch, classifier, candidate_labels):\n",
        "    \"\"\"\n",
        "    Classify paragraphs and titles in the given batch.\n",
        "\n",
        "    Args:\n",
        "        batch (dict): A batch of data containing speeches and titles.\n",
        "        classifier: The zero-shot classifier.\n",
        "        candidate_labels (list): Detailed labels linking specific issues to broader categories.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the classification results.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for speech, title, speaker, date, speech_id in tqdm(zip(batch['Speech'], batch['Title'], batch['Speaker'], batch['Date'], batch['SpeechID']), total=len(batch['Speech']), desc=\"Classifying Text\"):\n",
        "        # Classify the title\n",
        "        title_label, title_score = classify_text(title, classifier, candidate_labels) if title else (None, None)\n",
        "        results.append({'SpeechID': speech_id, 'Speaker': speaker, 'Date': date, 'Original_Title': title, 'Text': title, 'Label': title_label, 'Score': title_score, 'Type': 'Title'})\n",
        "\n",
        "        # Classify each paragraph in the speech\n",
        "        if speech:\n",
        "            paragraphs = speech.split('\\n\\n')\n",
        "            for paragraph in paragraphs:\n",
        "                if paragraph.strip():  # Ensure paragraph is not empty\n",
        "                    label, score = classify_text(paragraph, classifier, candidate_labels)\n",
        "                    results.append({'SpeechID': speech_id, 'Speaker': speaker, 'Date': date, 'Original_Title': title, 'Text': paragraph, 'Label': label, 'Score': score, 'Type': 'Paragraph'})\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    classifier = initialize_classifier()\n",
        "\n",
        "    if classifier:\n",
        "        # Load and prepare the dataset\n",
        "        #df = pd.read_csv('/content/drive/MyDrive/Presidents_Paper/Data_from_JupyterLab/All_noQ&A_250+NoSOTU.csv')\n",
        "        df = pd.read_csv('/content/drive/MyDrive/Presidents_Paper/Data_from_JupyterLab/All_noQ&A_100.csv')\n",
        "        df['SpeechID'] = range(1, len(df) + 1)\n",
        "        selected_df = df[['SpeechID', 'Speaker', 'Date', 'Title', 'Speech']]\n",
        "        dataset = Dataset.from_pandas(selected_df)\n",
        "\n",
        "        # Classify paragraphs and titles, creating the classified dataset\n",
        "        classified_dataset = dataset.map(lambda batch: classify_paragraphs_and_titles(batch, classifier=classifier, candidate_labels=candidate_labels), batched=True, batch_size=48)\n",
        "\n",
        "        # Convert the classified dataset back to a pandas DataFrame\n",
        "        classified_df = classified_dataset.to_pandas()\n",
        "\n",
        "        # Merge the original data with classification results\n",
        "        combined_df = pd.merge(selected_df, classified_df, on=['SpeechID', 'Speaker', 'Date'], how='left')\n",
        "\n",
        "        # Filter, sort, and save the results\n",
        "        filtered_df = combined_df.dropna(subset=['Label', 'Score'])\n",
        "        sorted_df = filtered_df.sort_values(by='Score', ascending=False)\n",
        "        sorted_df.to_csv('/content/drive/MyDrive/Presidents_Paper/CSV_files/Classified_Paragraphs_and_Titles.csv', sep=',', index=False)\n",
        "\n",
        "        print(\"Classification completed and saved to CSV.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV2Yjzz_4r3E"
      },
      "outputs": [],
      "source": [
        "# OLD Part 1: Uses Roberta to give health confidence scores to speech titles and paragraphs\n",
        "## Remember to switch to GPU, remember to ! pip install transformers, sentence transformers, and datasets.\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def initialize_classifier():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    classifier = pipeline(\"zero-shot-classification\", model=\"roberta-large-mnli\", device=device)\n",
        "    return classifier\n",
        "\n",
        "# Define candidate labels for classification\n",
        "candidate_labels = ['health', 'prevention and health', 'drug and alcohol abuse', 'environmental health', 'safety and health', 'mental health', 'disability and health', 'occupational health', 'sexual health']\n",
        "\n",
        "def create_batches(dataset, batch_size=128):\n",
        "    all_paragraphs = []\n",
        "    all_metadata = []\n",
        "    for index, row in dataset.iterrows():\n",
        "        paragraphs = [p for p in row['Speech'].split('\\n') if p.strip()]\n",
        "        for paragraph in paragraphs:\n",
        "            all_paragraphs.append(paragraph)\n",
        "            all_metadata.append((row['SpeechID'], row['Speaker'], row['Date'], row['Title']))\n",
        "    # Create batches\n",
        "    paragraph_batches = [all_paragraphs[i:i+batch_size] for i in range(0, len(all_paragraphs), batch_size)]\n",
        "    metadata_batches = [all_metadata[i:i+batch_size] for i in range(0, len(all_metadata), batch_size)]\n",
        "    return paragraph_batches, metadata_batches\n",
        "\n",
        "def classify_batches(classifier, paragraph_batches, metadata_batches, candidate_labels):\n",
        "    results = []\n",
        "    for paragraphs, metadata in tqdm(zip(paragraph_batches, metadata_batches), total=len(paragraph_batches), desc=\"Classifying Batches\"):\n",
        "        classifications = classifier(paragraphs, candidate_labels, multi_label=True)\n",
        "        for meta, classification in zip(metadata, classifications):\n",
        "            speech_id, speaker, date, title = meta\n",
        "            # Extract and format classification results\n",
        "            for label, score in zip(classification['labels'], classification['scores']):\n",
        "                results.append({\n",
        "                    'SpeechID': speech_id,\n",
        "                    'Speaker': speaker,\n",
        "                    'Date': date,\n",
        "                    'Original_Title': title,\n",
        "                    'Paragraph': paragraphs,\n",
        "                    label: score\n",
        "                })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    classifier = initialize_classifier()\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Presidents_Paper/Data_from_JupyterLab/All_noQ&A_100.csv')\n",
        "    df['SpeechID'] = range(1, len(df) + 1)\n",
        "    selected_df = df[['SpeechID', 'Speaker', 'Date', 'Title', 'Speech']].copy()\n",
        "    selected_df.loc[:, 'SpeechID'] = selected_df['SpeechID'].astype(str)\n",
        "\n",
        "    paragraph_batches, metadata_batches = create_batches(selected_df, batch_size=32)\n",
        "    classified_df = classify_batches(classifier, paragraph_batches, metadata_batches, candidate_labels)\n",
        "\n",
        "    classified_df.to_csv('/content/drive/MyDrive/Presidents_Paper/CSV_files/Classified_Paragraphs_and_Titles.csv', sep=',', index=False)\n",
        "    print(\"Classification completed and saved to CSV.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4co6Ue9N4r0c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB00uApL4rxr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBKXm-WP4rue"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yi4dAJHJ4rrH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dxPHvqX4rdU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_pxfRWC4rUf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efVmr9N64rGr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbVaUKeWZxzQ"
      },
      "outputs": [],
      "source": [
        "# OLD Part 1: Uses Roberta to give health confidence scores to speech titles and paragraphs, Remember to switch to GPU, remember to ! pip install transformers, sentence transformers, and datasets\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from datasets import Dataset\n",
        "\n",
        "# Function to initialize the Zero-Shot Classifier\n",
        "def initialize_classifier():\n",
        "    \"\"\"\n",
        "    Initialize the zero-shot classification pipeline.\n",
        "\n",
        "    Returns:\n",
        "        classifier: The initialized zero-shot classifier.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        classifier = pipeline(\"zero-shot-classification\", model=\"roberta-large-mnli\", device=0)\n",
        "        return classifier\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing the classifier: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to classify text\n",
        "\n",
        "def classify_text(text, classifier, candidate_labels):\n",
        "    \"\"\"\n",
        "    Classify the given text with detailed health categorization.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to classify.\n",
        "        classifier: The zero-shot classifier.\n",
        "        candidate_labels (list): Detailed labels linking specific issues to broader categories.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Contains the best matching label and its score.\n",
        "    \"\"\"\n",
        "    # Using combined and specific labels for nuanced classification\n",
        "    result = classifier(text, candidate_labels)\n",
        "    best_label = result['labels'][0]\n",
        "    best_score = result['scores'][0]\n",
        "    return best_label, best_score\n",
        "\n",
        "# Specifying nuanced candidate labels\n",
        "candidate_labels = ['health', 'prevention and health', 'drug and alcohol abuse', 'environmental health', 'safety and health', 'mental health', 'disability and health', 'occupational health', 'sexual health']\n",
        "\n",
        "# Function to classify paragraphs and titles in batches\n",
        "def classify_paragraphs_and_titles(batch):\n",
        "    \"\"\"\n",
        "    Classify paragraphs and titles in the given batch.\n",
        "\n",
        "    Args:\n",
        "        batch (dict): A batch of data containing speeches and titles.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the classification results.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for idx, (speech, title, speaker, date, speech_id) in enumerate(zip(batch['Speech'], batch['Title'], batch['Speaker'], batch['Date'], batch['SpeechID'])):\n",
        "        # Classify the title\n",
        "        title_label, title_score = classify_text(title, classifier) if title else (None, None)\n",
        "        results.append({'SpeechID': speech_id, 'Speaker': speaker, 'Date': date, 'Original_Title': title, 'Text': title, 'Label': title_label, 'Score': title_score, 'Type': 'Title'})\n",
        "\n",
        "        # Classify each paragraph in the speech\n",
        "        if speech:\n",
        "            paragraphs = speech.split('\\n\\n')\n",
        "            for paragraph in paragraphs:\n",
        "                if paragraph.strip():  # Check if the paragraph is not empty\n",
        "                    label, score = classify_text(paragraph, classifier)\n",
        "                    results.append({'SpeechID': speech_id, 'Speaker': speaker, 'Date': date, 'Original_Title': title, 'Text': paragraph, 'Label': label, 'Score': score, 'Type': 'Paragraph'})\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    classifier = initialize_classifier()\n",
        "\n",
        "    if classifier:\n",
        "        # Load the dataset\n",
        "        df = pd.read_csv('/content/drive/MyDrive/Presidents_Paper/Data_from_JupyterLab/All_noQ&A_250+NoSOTU.csv')\n",
        "\n",
        "        # Add a unique identifier for each speech\n",
        "        df['SpeechID'] = range(1, len(df) + 1)\n",
        "\n",
        "        selected_df = df[['SpeechID', 'Speaker', 'Date', 'Title', 'Speech']]\n",
        "\n",
        "        # Convert to a Dataset and classify\n",
        "        dataset = Dataset.from_pandas(selected_df)\n",
        "        classified_dataset = dataset.map(classify_paragraphs_and_titles, batched=True, batch_size=48)\n",
        "        classified_df = classified_dataset.to_pandas()\n",
        "\n",
        "        # Merge the original data with the classification results\n",
        "        combined_df = pd.merge(selected_df, classified_df, on=['SpeechID', 'Speaker', 'Date'], how='left')\n",
        "\n",
        "        # Further processing: Filter, Sort, and Save\n",
        "        filtered_df = combined_df.dropna(subset=['Label', 'Score'])\n",
        "        sorted_df = filtered_df.sort_values(by='Score', ascending=False)\n",
        "        sorted_df.to_csv('/content/drive/MyDrive/Presidents_Paper/CSV_files/Classified_Paragraphs_and_Titles.csv', sep=',', index=False)\n",
        "\n",
        "        print(\"Classified paragraphs and titles saved to CSV.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1rpM-Ls1t6h3h5F5z4QK4zvexGpSiU6ix",
      "authorship_tag": "ABX9TyMKQEO3fIBp+NTyd6zEFnmg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}